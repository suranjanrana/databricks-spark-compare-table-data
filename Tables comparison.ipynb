{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20109a2-7a5e-4ab4-bad3-f0fb3d0b2139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG dev;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19196a0c-108a-4bcb-9481-cea07714aebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import combinations\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4524fac3-3a20-4da3-ac01-8224ba37e1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TableMetadata:\n",
    "    def __init__(self, raw_table_name):\n",
    "        self.full_table_name = self._resolve_name(raw_table_name)\n",
    "        self._count = None\n",
    "        self._columns = None\n",
    "\n",
    "\n",
    "    def _resolve_name(self, raw_table_name: str) -> str:\n",
    "        parts = raw_table_name.split(\".\")\n",
    "        length = len(parts)\n",
    "\n",
    "        if length == 3: return raw_table_name\n",
    "        \n",
    "        try:\n",
    "            current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "            schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "        except:\n",
    "            raise Exception(\"Unable to determine current catalog\")\n",
    "        \n",
    "        if length == 2: return f\"{current_catalog}.{parts[0]}.{parts[1]}\"\n",
    "        if length == 1: return f\"{current_catalog}.{schema}.{parts[0]}\"\n",
    "        raise ValueError(f\"Invalid table name format: {raw_table_name}\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def row_count(self) -> int:\n",
    "        if self._count is None:\n",
    "            try:\n",
    "                self._count = spark.sql(f\"DESCRIBE DETAIL {self.full_table_name}\") \\\n",
    "                    .select(\"numRecords\").collect()[0][0]\n",
    "            except:\n",
    "                self._count = spark.read.table(self.full_table_name).count()\n",
    "        return self._count\n",
    "\n",
    "\n",
    "    @property\n",
    "    def columns(self) -> List[str]:\n",
    "        if self._columns is None:\n",
    "            self._columns = spark.read.table(self.full_table_name).columns\n",
    "        return self._columns\n",
    "\n",
    "\n",
    "    def read_clean(self, select_cols: List[str]) -> DataFrame:\n",
    "        return spark.read.table(self.full_table_name).select(select_cols)\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"<Table: {self.full_table_name}>\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80dca9e-52f2-4bbd-ae6d-26aba049b3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class KeyDiscoverer:\n",
    "    @staticmethod\n",
    "    def discover_primary_keys(df: DataFrame, candidates: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Identifies the best column(s) to use as a unique identifier\n",
    "        \"\"\"\n",
    "        print(\"\\tDiscovery: Analysing columns...\")\n",
    "        sample_rows = df.count()\n",
    "\n",
    "        exprs = [F.countDistinct(c).alias(c) for c in candidates]\n",
    "        distinct_counts = df.agg(*exprs).collect()[0].asDict()\n",
    "\n",
    "        keys = [col for col, count in distinct_counts.items() if count == sample_rows]\n",
    "        valid_keys = [k for k in keys if df.filter(F.col(k).isNull()).count() == 0]\n",
    "\n",
    "        if valid_keys:\n",
    "            best_key = next((k for k in valid_keys if \"id\" in k.lower()), valid_keys[0])\n",
    "            print(f\"\\tKeys Identified: ['{best_key}']\")\n",
    "            return [best_key]\n",
    "        \n",
    "        print(\"\\tNo single key found. Checking composite keys...\")\n",
    "\n",
    "        sorted_cols = sorted(distinct_counts, key=distinct_counts.get, reverse=True)\n",
    "        high_cardinality_cols = sorted_cols[:5]\n",
    "        \n",
    "        for combo in combinations(high_cardinality_cols, 3):\n",
    "            if df.select(combo).distinct().count() == sample_rows:\n",
    "                print(f\"\\tComposite Key Verified: {list(combo)}\")\n",
    "                return list(combo)\n",
    "        \n",
    "        print(\" \\WARNING: No unique key found. Using best-guess.\")\n",
    "        return sorted_cols[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a50810-bb53-4db8-a2e7-3ba8774ecb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TableComparator:\n",
    "    def __init__(self, source_table_name: str, target_table_name: str, primary_keys: Optional[List[str]] = None):\n",
    "        self.source = TableMetadata(source_table_name)\n",
    "        self.target = TableMetadata(target_table_name)\n",
    "        self.primary_keys = primary_keys\n",
    "        self.failed_rows_df = None\n",
    "\n",
    "        self.MAX_SAMPLE_SIZE = 10000\n",
    "\n",
    "\n",
    "    def _identify_common_columns(self) -> List[str]:\n",
    "        common_cols = set(self.source.columns).intersection(set(self.target.columns))\n",
    "        return sorted(list(common_cols))\n",
    "\n",
    "\n",
    "    def _sample_data(self):\n",
    "        common_columns = self._identify_common_columns()\n",
    "        source_count = self.source.row_count\n",
    "        \n",
    "        if source_count > self.MAX_SAMPLE_SIZE:\n",
    "            print(\"\\tLarge table. ({self.source_count} rows). Sampling...\")\n",
    "            fraction = self.MAX_SAMPLE_SIZE / self.source_count\n",
    "            df_source = self.source.read_clean(common_columns) \\\n",
    "                .sample(withReplacement=False, fraction=fraction, seed=42) \\\n",
    "                .alias(\"source\")\n",
    "        else:\n",
    "            print(\"\\tSmall table. Using FULL comparison.\")\n",
    "            df_source = self.source.read_clean(common_columns).alias(\"source\")\n",
    "        \n",
    "        if self.primary_keys:\n",
    "            keys = self.primary_keys\n",
    "        else:\n",
    "            keys = KeyDiscoverer.discover_primary_keys(df_source, common_columns)\n",
    "\n",
    "        df_target = self.target.read_clean(common_columns) \\\n",
    "            .join(df_source.select(keys), on=keys, how=\"inner\") \\\n",
    "            .alias(\"target\")\n",
    "\n",
    "        return df_source, df_target, keys, common_columns\n",
    "\n",
    "\n",
    "    def _execute_comparison(self, df_source, df_target, primary_keys, common_columns) -> bool:\n",
    "        data_cols = [c for c in common_columns if c not in primary_keys]\n",
    "\n",
    "        def _generate_hash(df):\n",
    "            return df.withColumn(\"fingerprint\", F.sha2(F.concat_ws(\"||\", *data_cols), 256))\n",
    "        \n",
    "        df_source_hashed = _generate_hash(df_source).select(*primary_keys, \"fingerprint\")\n",
    "        df_target_hashed = _generate_hash(df_target).select(*primary_keys, \"fingerprint\")\n",
    "\n",
    "        diff_source = df_source_hashed.exceptAll(df_target_hashed).withColumn(\"origin\", F.lit(\"source\"))\n",
    "        diff_target = df_target_hashed.exceptAll(df_source_hashed).withColumn(\"origin\", F.lit(\"target\"))\n",
    "\n",
    "        self.failed_rows_df = diff_source.unionByName(diff_target)\n",
    "        discrepancy_count = self.failed_rows_df.count()\n",
    "\n",
    "        if discrepancy_count == 0:\n",
    "            print(\"SUCCESS: Data matches.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"FAILED: Found {discrepancy_count} discrepencies.\")\n",
    "            diff_source.show(5)\n",
    "            return False\n",
    "    \n",
    "\n",
    "    def compare(self) -> bool:\n",
    "        print(f\"Starting Validatation: '{self.source.full_table_name}' vs '{self.target.full_table_name}'\")        \n",
    "        return self._execute_comparison(*self._sample_data())\n",
    "    \n",
    "\n",
    "    def get_mismatched_rows(self, limit=20):\n",
    "        if self.failed_rows_df is None or self.failed_rows_df.count() == 0:\n",
    "            print(\"No discrepancies to show.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Showing top {limit} mismatched rows...\")\n",
    "        full_details = spark.read.table(self.source.full_table_name) \\\n",
    "            .join(self.failed_rows_df, on=self.primary_keys, how=\"inner\") \\\n",
    "            .limit(limit)\n",
    "\n",
    "        return full_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29fff84a-fb63-419f-9df8-ceb9388c145c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validator = TableComparator(\"dev.test.employees\", \"dev.test.employees\")\n",
    "validator.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673a1946-981b-47c6-8b2f-34b2068073d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validator.get_mismatched_rows()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1692092196770098,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Tables comparison",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
