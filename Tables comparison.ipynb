{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20109a2-7a5e-4ab4-bad3-f0fb3d0b2139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG dev;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4524fac3-3a20-4da3-ac01-8224ba37e1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TableMetadata:\n",
    "    def __init__(self, raw_table_name):\n",
    "        parts = raw_table_name.split(\".\")\n",
    "\n",
    "        try:\n",
    "            current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "        except:\n",
    "            raise Exception(\"Unable to determine current catalog\")\n",
    "\n",
    "        match len(parts):\n",
    "            case 3:\n",
    "                self.catalog, self.schema, self.table = parts\n",
    "            case 2:\n",
    "                self.catalog = current_catalog\n",
    "                self.schema, self.table = parts\n",
    "            case 1:\n",
    "                self.catalog = current_catalog\n",
    "                self.schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "                self.table = raw_table_name\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid table name format: {raw_table_name}\")\n",
    "        \n",
    "        self.full_table_name = f\"{self.catalog}.{self.schema}.{self.table}\"\n",
    "    \n",
    "    def fetch_count(self):\n",
    "        try:\n",
    "            return spark.sql(f\"DESCRIBE DETAIL {self.full_table_name}\") \\\n",
    "                .select(\"numRecords\").collect()[0][0]\n",
    "        except:\n",
    "            return spark.read.table(self.full_table_name).count()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"<Table: {self.full_table_name}>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a50810-bb53-4db8-a2e7-3ba8774ecb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from itertools import combinations\n",
    "\n",
    "class TableComparator:\n",
    "    def __init__(self, source_table_name: str, target_table_name: str, primary_keys: list = None):\n",
    "        self.source_meta = TableMetadata(source_table_name)\n",
    "        self.target_meta = TableMetadata(target_table_name)\n",
    "        self.primary_keys = primary_keys if primary_keys else []\n",
    "\n",
    "        self.source_count = None\n",
    "        self.target_count = None\n",
    "\n",
    "        self.common_columns = sorted(self._identify_common_columns())\n",
    "        self.failed_rows_df = None\n",
    "\n",
    "        self.MAX_SAMPLE_SIZE = 10000\n",
    "\n",
    "\n",
    "    def _identify_common_columns(self):\n",
    "        cols_source = set(spark.table(self.source_meta.full_table_name).columns)\n",
    "        cols_target = set(spark.table(self.target_meta.full_table_name).columns)\n",
    "        return list(cols_source.intersection(cols_target))\n",
    "    \n",
    "\n",
    "    def _get_counts(self):\n",
    "        if self.source_count is None:\n",
    "            self.source_count = self.source_meta.fetch_count()\n",
    "        if self.target_count is None:\n",
    "            self.target_count = self.target_meta.fetch_count()\n",
    "\n",
    "\n",
    "    def _discover_primary_keys(self, df):\n",
    "        \"\"\"\n",
    "        Identifies the best column(s) to use as a unique identifier\n",
    "        \"\"\"\n",
    "        if self.primary_keys:\n",
    "            print(f\"\\tUsing provided Primary keys: {self.primary_keys}\")\n",
    "            return self.primary_keys\n",
    "        \n",
    "        print(\"\\tDiscovery: Analysing columns...\")\n",
    "        sample_rows = df.count()\n",
    "\n",
    "        exprs = [F.countDistinct(c).alias(c) for c in self.common_columns]\n",
    "        distinct_counts = df.agg(*exprs).collect()[0].asDict()\n",
    "\n",
    "        candidates = [col for col, count in distinct_counts.items() if count == sample_rows]\n",
    "        valid_keys = [k for k in candidates if df.filter(F.col(k).isNull()).count() == 0]\n",
    "\n",
    "        if valid_keys:\n",
    "            best_key = next((k for k in valid_keys if \"id\" in k.lower()), valid_keys[0])\n",
    "            self.primary_keys = [best_key]\n",
    "            print(f\"\\tKeys Identified: ['{best_key}']\")\n",
    "            return self.primary_keys\n",
    "        \n",
    "        print(\"\\tNo single key found. Checking composite keys...\")\n",
    "\n",
    "        high_cardinality_cols = [col for col, count in distinct_counts.items() if count > sample_rows * 0.9]\n",
    "        if len(high_cardinality_cols) < 2:\n",
    "            high_cardinality_cols = sorted(distinct_counts, key=distinct_counts.get, reverse=True)[:5]\n",
    "        \n",
    "        for combo in combinations(high_cardinality_cols, 2):\n",
    "            if df.select(combo).distinct().count() == sample_rows:\n",
    "                print(f\"\\tComposite Key Verified: {list(combo)}\")\n",
    "                return list(combo)\n",
    "        \n",
    "        print(\" \\WARNING: No unique key found. Using best-guess.\")\n",
    "        self.primary_keys = sorted(distinct_counts, key=distinct_counts.get, reverse=True)[:2]\n",
    "        return self.primary_keys\n",
    "\n",
    "\n",
    "    def _align_and_sample_data(self):\n",
    "        self._get_counts()\n",
    "\n",
    "        def read_clean(table_meta):\n",
    "            return spark.read.table(table_meta.full_table_name).select(self.common_columns)\n",
    "        \n",
    "        if self.source_count <= self.MAX_SAMPLE_SIZE:\n",
    "            print(\"\\tSmall table. Using FULL comparison.\")\n",
    "            df_source = read_clean(self.source_meta).alias(\"source\")\n",
    "\n",
    "            self.primary_keys = self._discover_primary_keys(df_source)\n",
    "\n",
    "            df_target = read_clean(self.target_meta).alias(\"target\")\n",
    "            return df_source, df_target\n",
    "        \n",
    "        print(\"\\tLarge table. ({self.source_count} rows). Sampling...\")\n",
    "        fraction = self.MAX_SAMPLE_SIZE / self.source_count\n",
    "\n",
    "        df_source = read_clean(self.source_meta) \\\n",
    "            .sample(withReplacement=False, fraction=fraction, seed=42) \\\n",
    "            .alias(\"source\")\n",
    "        \n",
    "        self.primary_keys = self._discover_primary_keys(df_source)\n",
    "\n",
    "        df_target = read_clean(self.target_meta) \\\n",
    "            .join(df_source.select(self.primary_keys), on=self.primary_keys, how=\"inner\") \\\n",
    "            .alias(\"target\")\n",
    "\n",
    "        return df_source, df_target\n",
    "\n",
    "\n",
    "    def execute_comparison(self):\n",
    "        print(f\"Starting Validatation: '{self.source_meta.full_table_name}' vs '{self.target_meta.full_table_name}'\")\n",
    "\n",
    "        if self.source_count != self.target_count:\n",
    "            print(f\"WARNING: Table sizes do not match. Source: {self.source_count} | Target: {self.target_count}\")\n",
    "        \n",
    "        df_source, df_target = self._align_and_sample_data()\n",
    "\n",
    "        data_cols = [c for c in self.common_columns if c not in self.primary_keys]\n",
    "\n",
    "        def _generate_fingerprint(df):\n",
    "            return df.withColumn(\"fingerprint\", F.sha2(F.concat_ws(\"||\", *data_cols), 256))\n",
    "        \n",
    "        df_source_hashed = _generate_fingerprint(df_source).select(*self.primary_keys, \"fingerprint\")\n",
    "        df_target_hashed = _generate_fingerprint(df_target).select(*self.primary_keys, \"fingerprint\")\n",
    "\n",
    "        diff_source = df_source_hashed.exceptAll(df_target_hashed).withColumn(\"origin\", F.lit(\"source\"))\n",
    "        diff_target = df_target_hashed.exceptAll(df_source_hashed).withColumn(\"origin\", F.lit(\"target\"))\n",
    "\n",
    "        self.failed_rows_df = diff_source.unionByName(diff_target)\n",
    "        discrepancy_count = self.failed_rows_df.count()\n",
    "\n",
    "        if discrepancy_count == 0:\n",
    "            print(\"SUCCESS: Data matches.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"FAILED: Found {discrepancy_count} discrepencies.\")\n",
    "            diff_source.show(5)\n",
    "            return False\n",
    "    \n",
    "\n",
    "    def get_mismatched_rows(self, limit=20):\n",
    "        if self.failed_rows_df is None or self.failed_rows_df.count() == 0:\n",
    "            print(\"No discrepancies to show.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Showing top {limit} mismatched rows...\")\n",
    "        full_details = spark.read.table(self.source_meta.full_table_name) \\\n",
    "            .join(self.failed_rows_df, on=self.primary_keys, how=\"inner\") \\\n",
    "            .limit(limit)\n",
    "\n",
    "        return full_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29fff84a-fb63-419f-9df8-ceb9388c145c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validator = TableComparator(\"dev.test.employees\", \"dev.test.employees\")\n",
    "validator.execute_comparison()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673a1946-981b-47c6-8b2f-34b2068073d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validator.get_mismatched_rows()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1692092196770098,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Tables comparison",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
