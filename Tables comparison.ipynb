{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20109a2-7a5e-4ab4-bad3-f0fb3d0b2139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG dev;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4524fac3-3a20-4da3-ac01-8224ba37e1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TableDetails:\n",
    "    def __init__(self, table):\n",
    "        parts = table.split(\".\")\n",
    "        match len(parts):\n",
    "            case 3:\n",
    "                self.catalog, self.schema, self.table_name = parts\n",
    "            case 2:\n",
    "                self.catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "                self.schema, self.table_name = parts\n",
    "            case 1:\n",
    "                self.catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "                self.schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "                self.table_name = table\n",
    "            case _:\n",
    "                raise Exception(f\"Invalid table name: {table}\")\n",
    "        self.three_level_namespace = f\"{self.catalog}.{self.schema}.{self.table_name}\"\n",
    "    \n",
    "    def get_details(self):\n",
    "        return spark.sql(f\"DESCRIBE DETAIL {self.three_level_namespace}\").collect()[0].asDict()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"TableStatus(table_name='{self.table_name}', columns={self.columns}, count={self.count})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a50810-bb53-4db8-a2e7-3ba8774ecb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "class TableComparator:\n",
    "    def __init__(self, expected_table: str, actual_table: str):\n",
    "        self.expected_table = TableDetails(expected_table)\n",
    "        self.actual_table = TableDetails(actual_table)\n",
    "\n",
    "        self.common_cols = sorted(self.get_commmon_cols(self.expected_table.three_level_namespace, self.actual_table.three_level_namespace))\n",
    "        self.keys = []\n",
    "\n",
    "        self.MAX_SAMPLE_SIZE = 10000\n",
    "\n",
    "        self.expected_row_count = self.get_row_count(self.expected_table)\n",
    "        self.actual_table_row_num = self.get_row_count(self.actual_table)\n",
    "    \n",
    "    def get_row_count(self, table):\n",
    "        return spark.read.table(table.three_level_namespace).count()\n",
    "    \n",
    "    def get_commmon_cols(self, table1, table2):\n",
    "        columns = spark.table(table1).columns\n",
    "        common_cols = [c for c in columns if c in spark.table(table2).columns]\n",
    "        return common_cols\n",
    "    \n",
    "    def _discover_keys(self, df):\n",
    "        columns = self.common_cols\n",
    "        distinct_counts = [df.select(c).distinct().count() for c in columns]\n",
    "\n",
    "        candidate_keys = [columns[i] for i in range(len(columns)) if distinct_counts[i] == self.expected_row_count]\n",
    "\n",
    "        if not candidate_keys:\n",
    "            max_col = columns[0]\n",
    "            max_2nd_col = columns[1]\n",
    "            max_distinct_count = distinct_counts[0]\n",
    "            max_2nd_distinct_count = distinct_counts[1]\n",
    "            for i in range(len(columns)):\n",
    "                if distinct_counts[i] >= max_distinct_count:\n",
    "                    max_distinct_count = distinct_counts[i]\n",
    "                    max_col = columns[i]\n",
    "                elif distinct_counts[i] >= max_2nd_distinct_count:\n",
    "                    max_2nd_distinct_count = distinct_counts[i]\n",
    "                    max_2nd_col = columns[i]\n",
    "            candidate_keys = [max_col, max_2nd_col]\n",
    "                    \n",
    "        return candidate_keys\n",
    "\n",
    "    def _get_samples(self, expected, actual):\n",
    "        expected_table_row_count = self.expected_row_count\n",
    "\n",
    "        sample_size = min(expected_table_row_count, self.MAX_SAMPLE_SIZE)\n",
    "\n",
    "        expected_df_sample = spark.read.table(expected.three_level_namespace).sample(withReplacement=False, fraction=sample_size / expected_table_row_count).alias(\"expected\")\n",
    "\n",
    "        self.keys = self._discover_keys(expected_df_sample)\n",
    "\n",
    "        actual_df_sample = spark.read.table(actual.three_level_namespace).join(expected_df_sample.select(self.keys), on=self.keys, how=\"inner\").alias(\"actual\")\n",
    "        return expected_df_sample, actual_df_sample\n",
    "    \n",
    "    def compare(self):\n",
    "        expected_df_sample, actual_df_sample = self._get_samples(self.expected_table, self.actual_table)\n",
    "\n",
    "        data_cols = self.common_cols\n",
    "\n",
    "        def add_hash(df):\n",
    "            return df.withColumn(\"row_hash\", F.sha2(F.concat_ws(\"||\", *data_cols), 256))\n",
    "        \n",
    "        expected_df_hashed = add_hash(expected_df_sample).select(*self.keys, \"row_hash\")\n",
    "        actual_df_hashed = add_hash(actual_df_sample).select(*self.keys, \"row_hash\")\n",
    "\n",
    "        diff_expected = expected_df_hashed.exceptAll(actual_df_hashed)\n",
    "        diff_actual = actual_df_hashed.exceptAll(expected_df_hashed)\n",
    "\n",
    "        issue_count = diff_expected.count() - diff_actual.count()\n",
    "\n",
    "        if issue_count == 0:\n",
    "            print(\"SUCCESS\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"FAILED: Found {issue_count} discrepencies\")\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29fff84a-fb63-419f-9df8-ceb9388c145c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tc = TableComparator(\"dev.test.employees\", \"dev.test.employees\")\n",
    "tc.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673a1946-981b-47c6-8b2f-34b2068073d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7508632266523300,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Tables comparison",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
