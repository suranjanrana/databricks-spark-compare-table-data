{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20109a2-7a5e-4ab4-bad3-f0fb3d0b2139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG dev;\n",
    "USE SCHEMA test;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19196a0c-108a-4bcb-9481-cea07714aebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import combinations\n",
    "from typing import List, Optional, Tuple, Literal\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4524fac3-3a20-4da3-ac01-8224ba37e1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TableMetadata:\n",
    "    def __init__(self, table_name):\n",
    "        self.full_name = self._resolve_name(table_name)\n",
    "        self._count = None\n",
    "\n",
    "\n",
    "    def _resolve_name(self, raw_name: str) -> str:\n",
    "        parts = raw_name.split(\".\")\n",
    "        length = len(parts)\n",
    "\n",
    "        if length == 3: return raw_name\n",
    "        \n",
    "        try:\n",
    "            current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "            current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "        except:\n",
    "            raise Exception(\"Unable to determine current catalog\")\n",
    "        \n",
    "        if length == 2: return f\"{current_catalog}.{parts[0]}.{parts[1]}\"\n",
    "        if length == 1: return f\"{current_catalog}.{current_schema}.{parts[0]}\"\n",
    "        raise ValueError(f\"Invalid table name format: {raw_name}\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def row_count(self) -> int:\n",
    "        if self._count is None:\n",
    "            try:\n",
    "                self._count = spark.sql(f\"DESCRIBE DETAIL {self.full_name}\") \\\n",
    "                    .select(\"numRecords\").collect()[0][0]\n",
    "            except:\n",
    "                self._count = spark.read.table(self.full_name).count()\n",
    "        return self._count\n",
    "\n",
    "\n",
    "    @property\n",
    "    def columns(self) -> List[str]:\n",
    "        return spark.read.table(self.full_name).columns\n",
    "\n",
    "\n",
    "    def read_selected(self, cols: List[str]) -> DataFrame:\n",
    "        return spark.read.table(self.full_name).select(cols)\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Table: {self.full_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80dca9e-52f2-4bbd-ae6d-26aba049b3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class KeyDiscoverer:\n",
    "    @staticmethod\n",
    "    def find_keys(df: DataFrame, candidates: List[str]) -> Tuple[List[str], bool]:\n",
    "        \"\"\"\n",
    "        Identifies the best column(s) to use as a unique identifier\n",
    "        \"\"\"\n",
    "        print(\"\\tDiscovery: Analysing columns...\")\n",
    "        sample_count = df.count()\n",
    "\n",
    "        exprs = [F.countDistinct(c).alias(c) for c in candidates]\n",
    "        distinct_counts = df.agg(*exprs).collect()[0].asDict()\n",
    "\n",
    "        keys = [col for col, count in distinct_counts.items() if count == sample_count]\n",
    "        valid_keys = [k for k in keys if df.filter(F.col(k).isNull()).count() == 0]\n",
    "\n",
    "        if valid_keys:\n",
    "            best_key = next((k for k in valid_keys if \"id\" in k.lower()), valid_keys[0])\n",
    "            print(f\"\\tSingle key Identified: ['{best_key}']\")\n",
    "            return [best_key], True\n",
    "        \n",
    "        print(\"\\tNo single key found. Checking composite keys...\")\n",
    "        sorted_cols = sorted(distinct_counts, key=distinct_counts.get, reverse=True)\n",
    "        top_candidates = sorted_cols[:5]\n",
    "        \n",
    "        for combo in combinations(top_candidates, 3):\n",
    "            if df.select(*combo).distinct().count() == sample_count:\n",
    "                print(f\"\\tComposite Key Verified: {list(combo)}\")\n",
    "                return list(combo), True\n",
    "        \n",
    "        print(\"\\tWARNING: No unique key found.\")\n",
    "        return [], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a50810-bb53-4db8-a2e7-3ba8774ecb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TableComparator:\n",
    "    history = []\n",
    "\n",
    "    def __init__(self, source_table: str, target_table: str, keys: Optional[List[str]] = None):\n",
    "        self.source = TableMetadata(source_table)\n",
    "        self.target = TableMetadata(target_table)\n",
    "        self.keys = keys\n",
    "\n",
    "        self.source_count = None\n",
    "        self.join_keys = []\n",
    "        self.is_keyless_mode = False\n",
    "\n",
    "        self._df_mismatch_source = None\n",
    "        self._df_mismatch_target = None\n",
    "        self._df_mismatches_merged = None\n",
    "\n",
    "        self._count_source_only = 0\n",
    "        self._count_target_only = 0\n",
    "        self.rows_analyzed = 0\n",
    "        \n",
    "        self.MAX_SAMPLE_SIZE = 10000\n",
    "\n",
    "\n",
    "    def _get_common_columns(self) -> List[str]:\n",
    "        common_cols = set(self.source.columns).intersection(set(self.target.columns))\n",
    "        return sorted(list(common_cols))\n",
    "\n",
    "\n",
    "    def _log_result(self, status: str, mode: str):\n",
    "        record = {\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"source\": self.source.full_name,\n",
    "            \"target\": self.target.full_name,\n",
    "            \"status\": status,\n",
    "            \"mode\": mode,\n",
    "            \"keys\": str(self.join_keys) if self.join_keys else \"N/A\",\n",
    "            \"source_mismatch_count\": self._count_source_only,\n",
    "            \"target_mismatch_count\": self._count_target_only if self._count_target_only != -1 else \"N/A\"\n",
    "        }\n",
    "        TableComparator.history.append(record)\n",
    "\n",
    "\n",
    "    def compare(self, full_scan=False) -> bool:\n",
    "        print(f\"\\n\\nStarting Validatation: '{self.source.full_name}' vs '{self.target.full_name}'\")\n",
    "        self.source_count = self.source.row_count\n",
    "        common_columns = self._get_common_columns()\n",
    "\n",
    "        is_sampling = (self.source_count > self.MAX_SAMPLE_SIZE) and (not full_scan)\n",
    "        \n",
    "        if is_sampling:\n",
    "            print(f\"\\tLarge table ({self.source_count} rows). Sampling...\")\n",
    "            fraction = self.MAX_SAMPLE_SIZE / self.source_count\n",
    "            df_source = self.source.read_selected(common_columns) \\\n",
    "                .sample(withReplacement=False, fraction=fraction, seed=42) \\\n",
    "                .alias(\"source\")\n",
    "        else:\n",
    "            print(f\"\\tSmall table. Using FULL comparison ({self.source_count} rows).\")\n",
    "            df_source = self.source.read_selected(common_columns).alias(\"source\")\n",
    "        \n",
    "        self.rows_analyzed = df_source.count()\n",
    "        print(f\"\\tActual rows Analyzed: {self.rows_analyzed}\")\n",
    "        \n",
    "        if self.keys:\n",
    "            self.join_keys, is_trusted = self.keys, True\n",
    "        else:\n",
    "            self.join_keys, is_trusted = KeyDiscoverer.find_keys(df_source, common_columns)\n",
    "        \n",
    "        if is_trusted:\n",
    "            self.is_keyless_mode = False\n",
    "            print(f\"\\tMode: Key-based validation. (Hash comparison) (Keys: {self.join_keys})\")\n",
    "            df_target = self.target.read_selected(common_columns) \\\n",
    "                .join(df_source.select(self.join_keys), on=self.join_keys, how=\"inner\") \\\n",
    "                .alias(\"target\")\n",
    "            \n",
    "            return self._run_keyed_comparison(df_source, df_target, common_columns)\n",
    "        else:\n",
    "            self.is_keyless_mode = True\n",
    "            print(\"\\tMode: Keyless validation. (Set Difference)\")\n",
    "            return self._run_keyless_comparison(df_source, common_columns, is_sampling)\n",
    "\n",
    "\n",
    "    def _run_keyed_comparison(self, df_source, df_target, cols) -> bool:\n",
    "        hash_cols = [c for c in cols if c not in self.join_keys]\n",
    "\n",
    "        def _hash(df):\n",
    "            return df.withColumn(\"fingerprint\", F.sha2(F.concat_ws(\"||\", *hash_cols), 256))\n",
    "        \n",
    "        df_source_hashed = _hash(df_source).select(*self.join_keys, \"fingerprint\")\n",
    "        df_target_hashed = _hash(df_target).select(*self.join_keys, \"fingerprint\")\n",
    "\n",
    "        self._df_mismatch_source = df_source_hashed.exceptAll(df_target_hashed).withColumn(\"origin\", F.lit(\"source\"))\n",
    "        self._df_mismatch_target = df_target_hashed.exceptAll(df_source_hashed).withColumn(\"origin\", F.lit(\"target\"))\n",
    "\n",
    "        self._count_source_only = self._df_mismatch_source.count()\n",
    "        self._count_target_only = self._df_mismatch_target.count()\n",
    "\n",
    "        self._df_mismatches_merged = self._df_mismatch_source.unionByName(self._df_mismatch_target)\n",
    "\n",
    "        if self._count_source_only + self._count_target_only == 0:\n",
    "            print(f\"SUCCESS: Data matches. {self.source_count} rows\")\n",
    "            self._log_result(\"PASS\", \"KEYED\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"FAILED:\")\n",
    "            print(f\"\\tSource mismatch: {self._count_source_only}\\n\\tTarget mismatch: {self._count_target_only}\\n\\tTotal rows in source: {self.source_count}\")\n",
    "            self._log_result(\"FAIL\", \"KEYED\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def _run_keyless_comparison(self, df_source, cols, is_sampling: bool) -> bool:\n",
    "        df_target = self.target.read_selected(cols).alias(\"target\")\n",
    "        \n",
    "        self._df_mismatch_source = df_source.exceptAll(df_target).withColumn(\"origin\", F.lit(\"source\"))\n",
    "        self._count_source_only = self._df_mismatch_source.count()\n",
    "\n",
    "        if is_sampling:\n",
    "            self._df_mismatch_target = None\n",
    "            self._count_target_only = -1\n",
    "\n",
    "            self._df_mismatches_merged = self._df_mismatch_source\n",
    "        else:\n",
    "            self._df_mismatch_target = df_target.exceptAll(df_source).withColumn(\"origin\", F.lit(\"target\"))\n",
    "            self._count_target_only = self._df_mismatch_target.count()\n",
    "\n",
    "            self._df_mismatches_merged = self._df_mismatch_src.unionByName(self._df_mismatch_target)\n",
    "        \n",
    "        failed = (self._count_source_only > 0) or (self._count_target_only > 0)\n",
    "\n",
    "        if not failed == 0:\n",
    "            print(f\"SUCCESS: Data matches. {self.source_count} rows\")\n",
    "            self._log_result(\"PASS\", \"KEYLESS\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"FAILED:\")\n",
    "            print(f\"\\tSource mismatch: {self._count_source_only}\\n\\tTarget mismatch: {self._count_target_only}\\n\\tTotal rows in source: {self.source_count}\")\n",
    "            self._log_result(\"FAIL\", \"KEYLESS\")\n",
    "            return False\n",
    "\n",
    "\n",
    "    def get_all_mismatches(self, limit: int = 100) -> DataFrame:\n",
    "        if self._df_mismatches_merged is None or self._df_mismatches_merged.count() == 0:\n",
    "            print(\"No discrepancies to show.\")\n",
    "            return None\n",
    "        \n",
    "        if self.is_keyless_mode:\n",
    "            return self._df_mismatches_merged.limit(limit)\n",
    "\n",
    "        print(f\"\\tFetching full row details for combined mismatches...\")\n",
    "\n",
    "        final_df = None\n",
    "\n",
    "        if self._count_source_only > 0:\n",
    "            df_source_errs = spark.read.table(self.source.full_name) \\\n",
    "                .join(self._df_mismatch_source, on=self.join_keys, how=\"inner\") \\\n",
    "                .limit(limit)\n",
    "            final_df = df_source_errs\n",
    "        \n",
    "        if self._count_target_only > 0:\n",
    "            df_target_errs = spark.read.table(self.target.full_name) \\\n",
    "                .join(self._df_mismatch_target, on=self.join_keys, how=\"inner\") \\\n",
    "                .limit(limit)\n",
    "            \n",
    "            if final_df:\n",
    "                cols = self._get_common_columns() + [\"origin\"]\n",
    "                final_df = final_df.select(cols).unionByName(df_target_errs.select(cols))\n",
    "            else:\n",
    "                final_df = df_target_errs\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "\n",
    "    def get_summary_report(self) -> DataFrame:\n",
    "        if self.source_count is None:\n",
    "            print(\"Summary not available. Please run `compare()` first.\")\n",
    "\n",
    "        print(\"Generating Summary Report...\")\n",
    "        target_count = self.target.row_count\n",
    "        row_diff = self.source_count - target_count\n",
    "\n",
    "        matched_count = self.rows_analyzed - self._count_source_only\n",
    "        match_percentage = (matched_count / self.source_count) * 100.0 if self.source_count > 0 else 0.0\n",
    "\n",
    "        data = [\n",
    "            (\"Total rows compared\", str(self.rows_analyzed)),\n",
    "            (\"Matched rows\", f\"{matched_count} ({match_percentage:.2f}%)\"),\n",
    "            (\"Source Rows (Total)\", str(self.source_count)),\n",
    "            (\"Target Rows (Total)\", str(target_count)),\n",
    "            (\"Total row count diff\", str(row_diff)),\n",
    "            (\"Mismatch in source\", str(self._count_source_only)),\n",
    "            (\"Mismatch in target\", \"N/A (Sampling)\" if self._count_target_only == -1 else str(self._count_target_only))\n",
    "        ]\n",
    "\n",
    "        if not self.is_keyless_mode and (self._count_source_only > 0 or self._count_target_only > 0):\n",
    "            modified_ids = self._df_mismatch_source.join(self._df_mismatch_target, on=self.join_keys, how=\"inner\") \\\n",
    "                .select(self.join_keys)\n",
    "            mod_count = modified_ids.count()\n",
    "\n",
    "            if mod_count > 0:\n",
    "                data.append((\"Mismatch rows (ID match, data diff)\", str(mod_count)))\n",
    "\n",
    "                common = self._get_common_columns()\n",
    "                source_data = self.source.read_selected(common).join(modified_ids, on=self.join_keys, how=\"inner\")\n",
    "                target_data = self.target.read_selected(common).join(modified_ids, on=self.join_keys, how=\"inner\")\n",
    "\n",
    "                cols_to_check = [c for c in common if c not in self.join_keys]\n",
    "                diff_exprs = [F.sum(F.when(F.col(f\"source.{c}\") != F.col(f\"target.{c}\"), 1).otherwise(0)).alias(c) for c in cols_to_check]\n",
    "\n",
    "                joined = source_data.alias(\"source\").join(target_data.alias(\"target\"), on=self.join_keys, how=\"inner\")\n",
    "                diff_sums = joined.agg(*diff_exprs).collect()[0].asDict()\n",
    "\n",
    "                sorted_diffs = sorted(diff_sums.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "                for col, count in sorted_diffs:\n",
    "                    if count > 0:\n",
    "                        data.append((f\"\\t Col '{col}' mismatched\", f\"{count} ({(count / mod_count) * 100:.1f}%)\"))\n",
    "\n",
    "        return spark.createDataFrame(data, [\"metric\", \"value\"])\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_history(cls) -> DataFrame:\n",
    "        if not cls.history:\n",
    "            print(\"No history available yet.\")\n",
    "            return None\n",
    "        return spark.createDataFrame(cls.history)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def clear_history(cls):\n",
    "        cls.history = []\n",
    "        print(\"History cleared.\")\n",
    "    \n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.source.full_name} vs {self.target.full_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29fff84a-fb63-419f-9df8-ceb9388c145c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "comparator = TableComparator(\"employees\", \"employees_2\")\n",
    "comparator.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea34137a-2f3e-4b38-9025-504fb3e7f99e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logs = TableComparator.get_history()\n",
    "logs.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "776a7984-4d7a-4913-a9b0-c81aeaec5ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "comparator.get_all_mismatches().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc12ab5b-aa88-487e-85d1-40a7cd066f26",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"metric\":284},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765997906997}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "comparator.get_summary_report().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1ecfa98-b3ad-4756-bd68-ec87fd05ed30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "comparator"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1692092196770098,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Tables comparison",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
