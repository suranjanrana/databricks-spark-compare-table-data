{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a20109a2-7a5e-4ab4-bad3-f0fb3d0b2139",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "USE CATALOG dev;\n",
    "USE SCHEMA test;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19196a0c-108a-4bcb-9481-cea07714aebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from itertools import combinations\n",
    "from typing import List, Optional, Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4524fac3-3a20-4da3-ac01-8224ba37e1fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TableMetadata:\n",
    "    def __init__(self, raw_table_name):\n",
    "        self.full_table_name = self._resolve_name(raw_table_name)\n",
    "        self._count = None\n",
    "        self._columns = None\n",
    "\n",
    "\n",
    "    def _resolve_name(self, raw_table_name: str) -> str:\n",
    "        parts = raw_table_name.split(\".\")\n",
    "        length = len(parts)\n",
    "\n",
    "        if length == 3: return raw_table_name\n",
    "        \n",
    "        try:\n",
    "            current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "            schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "        except:\n",
    "            raise Exception(\"Unable to determine current catalog\")\n",
    "        \n",
    "        if length == 2: return f\"{current_catalog}.{parts[0]}.{parts[1]}\"\n",
    "        if length == 1: return f\"{current_catalog}.{schema}.{parts[0]}\"\n",
    "        raise ValueError(f\"Invalid table name format: {raw_table_name}\")\n",
    "\n",
    "\n",
    "    @property\n",
    "    def row_count(self) -> int:\n",
    "        if self._count is None:\n",
    "            try:\n",
    "                self._count = spark.sql(f\"DESCRIBE DETAIL {self.full_table_name}\") \\\n",
    "                    .select(\"numRecords\").collect()[0][0]\n",
    "            except:\n",
    "                self._count = spark.read.table(self.full_table_name).count()\n",
    "        return self._count\n",
    "\n",
    "\n",
    "    @property\n",
    "    def columns(self) -> List[str]:\n",
    "        if self._columns is None:\n",
    "            self._columns = spark.read.table(self.full_table_name).columns\n",
    "        return self._columns\n",
    "\n",
    "\n",
    "    def read_clean(self, select_cols: List[str]) -> DataFrame:\n",
    "        return spark.read.table(self.full_table_name).select(select_cols)\n",
    "\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Table: {self.full_table_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80dca9e-52f2-4bbd-ae6d-26aba049b3d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class KeyDiscoverer:\n",
    "    @staticmethod\n",
    "    def discover_primary_keys(df: DataFrame, candidates: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Identifies the best column(s) to use as a unique identifier\n",
    "        \"\"\"\n",
    "        print(\"\\tDiscovery: Analysing columns...\")\n",
    "        sample_rows = df.count()\n",
    "\n",
    "        exprs = [F.countDistinct(c).alias(c) for c in candidates]\n",
    "        distinct_counts = df.agg(*exprs).collect()[0].asDict()\n",
    "\n",
    "        keys = [col for col, count in distinct_counts.items() if count == sample_rows]\n",
    "        valid_keys = [k for k in keys if df.filter(F.col(k).isNull()).count() == 0]\n",
    "\n",
    "        if valid_keys:\n",
    "            best_key = next((k for k in valid_keys if \"id\" in k.lower()), valid_keys[0])\n",
    "            print(f\"\\tKeys Identified: ['{best_key}']\")\n",
    "            return [best_key], True\n",
    "        \n",
    "        print(\"\\tNo single key found. Checking composite keys...\")\n",
    "        sorted_cols = sorted(distinct_counts, key=distinct_counts.get, reverse=True)\n",
    "        top_candidates = sorted_cols[:5]\n",
    "        \n",
    "        for combo in combinations(top_candidates, 3):\n",
    "            if df.select(*combo).distinct().count() == sample_rows:\n",
    "                print(f\"\\tComposite Key Verified: {list(combo)}\")\n",
    "                return list(combo), True\n",
    "        \n",
    "        print(\"\\tWARNING: No unique key found.\")\n",
    "        return [], False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52a50810-bb53-4db8-a2e7-3ba8774ecb1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TableComparator:\n",
    "    history = []\n",
    "\n",
    "    def __init__(self, source_table_name: str, target_table_name: str, primary_keys: Optional[List[str]] = None):\n",
    "        self.source = TableMetadata(source_table_name)\n",
    "        self.target = TableMetadata(target_table_name)\n",
    "        self.primary_keys = primary_keys\n",
    "        self.source_count = None\n",
    "\n",
    "        self._source_only_raw = None\n",
    "        self._target_only_raw = None\n",
    "        self._source_only_count = 0\n",
    "        self._target_only_count = 0\n",
    "        self.active_keys = []\n",
    "        self.is_keyless_mode = False\n",
    "\n",
    "        self.MAX_SAMPLE_SIZE = 10000\n",
    "\n",
    "\n",
    "    def _identify_common_columns(self) -> List[str]:\n",
    "        common_cols = set(self.source.columns).intersection(set(self.target.columns))\n",
    "        return sorted(list(common_cols))\n",
    "\n",
    "    def _log_result(self, status: str, mode: str, source_err: int, target_err: int):\n",
    "        record = {\n",
    "            \"source_table\": self.source.full_table_name,\n",
    "            \"target_table\": self.target.full_table_name,\n",
    "            \"status\": status,\n",
    "            \"mode\": mode,\n",
    "            \"keys_used\": str(self.active_keys) if self.active_keys else \"N/A\",\n",
    "            \"source_only_count\": source_err,\n",
    "            \"target_only_count\": target_err if target_err is not None else -1\n",
    "        }\n",
    "        TableComparator.history.append(record)\n",
    "\n",
    "    def compare(self) -> bool:\n",
    "        print(f\"\\n\\nStarting Validatation: '{self.source.full_table_name}' vs '{self.target.full_table_name}'\")\n",
    "        common_columns = self._identify_common_columns()\n",
    "        self.source_count = self.source.row_count\n",
    "        \n",
    "        if self.source_count > self.MAX_SAMPLE_SIZE:\n",
    "            print(f\"\\tLarge table. ({self.source_count} rows). Sampling...\")\n",
    "            fraction = self.MAX_SAMPLE_SIZE / self.source_count\n",
    "            df_source = self.source.read_clean(common_columns) \\\n",
    "                .sample(withReplacement=False, fraction=fraction, seed=42) \\\n",
    "                .alias(\"source\")\n",
    "        else:\n",
    "            print(\"\\tSmall table. Using FULL comparison.\")\n",
    "            df_source = self.source.read_clean(common_columns).alias(\"source\")\n",
    "        \n",
    "        if self.primary_keys:\n",
    "            self.active_keys, is_trusted = self.primary_keys, True\n",
    "        else:\n",
    "            self.active_keys, is_trusted = KeyDiscoverer.discover_primary_keys(df_source, common_columns)\n",
    "        \n",
    "        if is_trusted:\n",
    "            self.is_keyless_mode = False\n",
    "            print(\"\\tMode: Key based validation. (Hash comparison)\")\n",
    "            df_target = self.target.read_clean(common_columns) \\\n",
    "                .join(df_source.select(self.active_keys), on=self.active_keys, how=\"inner\") \\\n",
    "                .alias(\"target\")\n",
    "            \n",
    "            return self._execute_hash_comparison(df_source, df_target, self.active_keys, common_columns)\n",
    "        else:\n",
    "            self.is_keyless_mode = True\n",
    "            print(\"\\tMode: Keyless validation. (Set Difference)\")\n",
    "            return self._execute_keyless_comparison(df_source, common_columns)\n",
    "\n",
    "\n",
    "    def _execute_hash_comparison(self, df_source, df_target, primary_keys, common_columns) -> bool:\n",
    "        data_cols = [c for c in common_columns if c not in primary_keys]\n",
    "\n",
    "        def _generate_hash(df):\n",
    "            return df.withColumn(\"fingerprint\", F.sha2(F.concat_ws(\"||\", *data_cols), 256))\n",
    "        \n",
    "        df_source_hashed = _generate_hash(df_source).select(*primary_keys, \"fingerprint\")\n",
    "        df_target_hashed = _generate_hash(df_target).select(*primary_keys, \"fingerprint\")\n",
    "\n",
    "        self._source_only_raw = df_source_hashed.exceptAll(df_target_hashed).withColumn(\"origin\", F.lit(\"source\"))\n",
    "        self._target_only_raw = df_target_hashed.exceptAll(df_source_hashed).withColumn(\"origin\", F.lit(\"target\"))\n",
    "\n",
    "        self._source_only_count = self._source_only_raw.count()\n",
    "        self._target_only_count = self._target_only_raw.count()\n",
    "\n",
    "        if self._source_only_count + self._target_only_count == 0:\n",
    "            print(f\"SUCCESS: Data matches. {self.source_count} rows\")\n",
    "            self._log_result(\"PASS\", \"KEYED\", 0, 0)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"FAILED: {self._source_only_count + self._target_only_count} / {self.source_count} rows does not match.\")\n",
    "            self._log_result(\"FAIL\", \"KEYED\", self._source_only_count, self._target_only_count)\n",
    "            print(\"\\nSource:\")\n",
    "            self._source_only_raw.show(5)\n",
    "            print(\"\\nTarget:\")\n",
    "            self._target_only_raw.show(5)\n",
    "            return False\n",
    "\n",
    "\n",
    "    def _execute_keyless_comparison(self, df_source, common_columns) -> bool:\n",
    "        df_target = self.target.read_clean(common_columns).alias(\"target\")\n",
    "        \n",
    "        self._source_only_raw = df_source.exceptAll(df_target)\n",
    "        self._target_only_raw = None\n",
    "        self._source_only_count = self._source_only_raw.count()\n",
    "\n",
    "        if self._source_only_count == 0:\n",
    "            print(f\"SUCCESS: Data matches. {self.source_count} rows\")\n",
    "            self._log_result(\"PASS\", \"KEYLESS\", 0, None)\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"FAILED: {self._source_only_count} / {self.source_count} rows does not match.\")\n",
    "            self._log_result(\"FAIL\", \"KEYLESS\", self._source_only_count, None)\n",
    "            print(\"\\nSource:\")\n",
    "            self._source_only_raw.show(5)\n",
    "            return False\n",
    "\n",
    "\n",
    "    def get_source_only(self, limit: int = 20) -> DataFrame:\n",
    "        if self._source_only_raw is None or self._source_only_count == 0:\n",
    "            print(\"No discrepancies to show.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Showing top {limit} mismatched rows...\")\n",
    "        if self.is_keyless_mode:\n",
    "            return self._source_only_raw.limit(limit)\n",
    "        else:\n",
    "            keys = self.active_keys\n",
    "            return spark.read.table(self.source.full_table_name) \\\n",
    "                .join(self._source_only_raw, on=keys, how=\"inner\") \\\n",
    "                .limit(limit)\n",
    "\n",
    "\n",
    "    def get_target_only(self, limit: int = 20) -> DataFrame:\n",
    "        if self._target_only_raw is None or self._target_only_count == 0:\n",
    "            print(\"No discrepancies to show.\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Showing top {limit} mismatched rows...\")\n",
    "        if self.is_keyless_mode:\n",
    "            return spark.read.table(self.target.full_table_name).exceptAll(self._source_only_raw).limit(limit)\n",
    "        else:\n",
    "            keys = self.active_keys\n",
    "            return spark.read.table(self.target.full_table_name) \\\n",
    "                .join(self._target_only_raw, on=keys, how=\"inner\") \\\n",
    "                .limit(limit)\n",
    "\n",
    "\n",
    "    def get_error_rows(self, from_tbl: Literal[\"source\", \"target\"], limit: int = 20) -> DataFrame:\n",
    "        match from_tbl:\n",
    "            case \"source\":\n",
    "                self.get_source_only(limit)\n",
    "            case \"target\":\n",
    "                self.get_target_only(limit)\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid. Please enter either 'source' or 'target'\")\n",
    "        return True\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def get_history(cls) -> DataFrame:\n",
    "        if not cls.history:\n",
    "            print(\"No history available yet.\")\n",
    "            return None\n",
    "        return spark.createDataFrame(cls.history)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def clear_history(cls):\n",
    "        cls.history = []\n",
    "        print(\"History cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29fff84a-fb63-419f-9df8-ceb9388c145c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validator = TableComparator(\"employees\", \"employees\")\n",
    "validator.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673a1946-981b-47c6-8b2f-34b2068073d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "validator.get_error_rows(from_tbl=\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea34137a-2f3e-4b38-9025-504fb3e7f99e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "logs = TableComparator.get_history()\n",
    "logs.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1692092196770098,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Tables comparison",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
